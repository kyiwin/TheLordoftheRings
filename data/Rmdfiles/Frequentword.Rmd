---
title: "LR"
author: "Kyi Win"
date: "3/30/2019"
output:
  word_document: default
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:

```{r cars}
summary(cars)
```

## Including Plots

You can also embed plots, for example:

```{r pressure, echo=FALSE}
plot(pressure)
```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.

Create a text file
Created a R package for The Lord of the Rings text
```{r}
library(TheLordoftheRings)
#head(TheLordoftheRings)
```

Install and load the required packages

Install
```{r}
install.packages("tm")
install.packages("SnowballC") # for text stemming
install.packages("wordcloud") # word-cloud generator 
install.packages("RColorBrewer") # color palettes
```
Load
```{r}
library("tm")
library("SnowballC")
library("wordcloud")
library("RColorBrewer")
```

Load the text

load the data as a corpus
```{r}
text <- Corpus(VectorSource(TheLordoftheRings))
```
Inspect the content of the document
```{r}
#inspect(text)
```

```{r}
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
texts <- tm_map(text, toSpace, "/")
texts <- tm_map(text, toSpace, "@")
texts <- tm_map(text, toSpace, "\\|")
texts <- tm_map(text, toSpace, "?")
texts <- tm_map(text, toSpace, "!")
texts <- tm_map(text, toSpace, "," )
texts <- tm_map(text, toSpace, " " )
```
```{r}
texts <- tm_map(text, toSpace, "," )
texts <- tm_map(text, toSpace, " " )
```

```{r}
#inspect(head(texts))
```

```{r}
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
#docs <- tm_map(text, toSpace, "/")
#docs <- tm_map(text, toSpace, "@")
#docs <- tm_map(text, toSpace, "\\|")
docs <- tm_map(texts, toSpace, "\u0099")
```

```{r}
#inspect(docs)
```

```{r}
# Convert the text to lower case
docs <- tm_map(docs, content_transformer(tolower))
# Remove numbers
docs <- tm_map(docs, removeNumbers)
# Remove english common stopwords
#docs <- tm_map(docs, removeWords, stopwords("english"))
# Remove your own stop word
# specify your stopwords as a character vector
#docs <- tm_map(docs, removeWords, c("blabla1", "blabla2")) 
# Remove punctuations
docs <- tm_map(docs, removePunctuation)
# Eliminate extra white spaces
docs <- tm_map(docs, stripWhitespace)
# Text stemming
docs <- tm_map(docs, stemDocument)
```
```{r}
docs <- tm_map(text, toSpace, "\\|")
```

```{r}
#head(inspect(docs))
```

```{r}
stopWords <-stopwords("en")
stopWords
```



```{r}
docsv <- Corpus(VectorSource(docs))
```
```{r}
docsrsw <- tm_map(docsv, removeWords, stopwords('en'))
```

```{r}
#head(inspect(docsrsw))

```

```{r}
docs <- tm_map(docsrsw, stripWhitespace)
```

```{r}
#inspect(docs)
```

```{r}
dtm <- TermDocumentMatrix(docs)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
head(d, 10)
```

```{r}
set.seed(1234)
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
          max.words=200, random.order=FALSE, rot.per=0.35, 
          colors=brewer.pal(8, "Paired"))
```

Explore Frequent terms and their associations
```{r}
findFreqTerms(dtm, lowfreq = 200)
freqterm <- findFreqTerms(dtm, lowfreq = 200)
```


```{r}
barplot(d[1:10,]$freq, las = 2, names.arg = d[1:10,]$word,
        col ="navy", main ="Most frequent words",
        ylab = "Word frequencies")
```

```{r}
library(gutenbergr)
library(dplyr)
library(tidytext)  
theringsetsout <- Book1_TheRingsetsout %>%
  unnest_tokens(alreadycleaned, text) 
```
LDA
```{r}
text_dtm <- TermDocumentMatrix(dtm)
text_lda <- LDA(text_dtm, k = 2, method = "VEM", control = NULL)
text_lda
```
```{r}
library(tidytext)

ap_topics <- tidy(text_lda, matrix = "beta")
ap_topics
```

Sentiment Analysis

```{r}
Legolas <- function(string){
  temp <- tolower(string) 
  temp <- stringr::str_replace_all(temp, "[^a-zA-Z\\s]", " ") 
  temp <- stringr::str_replace_all(temp, "[\\s]+", " ") 
  temp <- stringr::str_split(temp, " ")[[1]]
  index <- which(temp == "")
  if(length(index) > 0){
    temp <- temp[-index]
    }
  return(temp)
}
```


```{r}
library(tidytext)
word <- get_sentiments("nrc")[,1]
sentiments <- get_sentiments("nrc")[,2]
  
for(i in word){
  for (j in docsrsw){
    if (j == i) {
      print(i)
    }
  }
}
 # filter(!is.na(sentiment))
 # count(sentiment, sort = TRUE)
```
```{r}
for (j in docsrsw){
  print(j)
}
```

```{r}
alreadycleaned <- Legolas(docs)
print(alreadycleaned)
```

```{r}
library(tidytext)
wordgs <- get_sentiments("nrc")[,1]
wordgs
```
```{r}
p <- get_sentiments("nrc")
print(p)
```


```{r}
library(tibble)
library(dplyr)
library(ggplot2)
library(memery)
library(magick)
table <- tibble(alreadycleaned)
table %>% right_join(p, c("alreadycleaned" = "word")) %>% group_by(sentiment) %>% summarise(wordcount = n()) %>% ungroup() %>% mutate(sentiment = reorder(sentiment, wordcount)) 


table %>% right_join(p, c("alreadycleaned" = "word")) %>% group_by(sentiment) %>% summarise(wordcount = n()) %>% ungroup() %>% mutate(sentiment = reorder(sentiment, wordcount)) %>%
  ggplot(aes(sentiment, wordcount, fill = -wordcount)) + geom_col() + guides(fill = FALSE)+ labs(x = NULL, y = "Word Count") + scale_y_continuous(limits = c(0, 15000)) + ggtitle("The Lord of the Rings Sentiment") + coord_flip()


sentimentpic <- table %>% right_join(p, c("alreadycleaned" = "word")) %>% group_by(sentiment) %>% summarise(wordcount = n()) %>% ungroup() %>% mutate(sentiment = reorder(sentiment, wordcount)) %>%
  ggplot(aes(sentiment, wordcount, fill = -wordcount)) + geom_col() + guides(fill = FALSE)+ labs(x = NULL, y = "Word Count") + scale_y_continuous(limits = c(0, 15000)) + ggtitle("The Lord of the Rings Sentiment") + coord_flip()

#ggsave("sentimentpic.jpg")
#join <- right_join(table, p , by = (table["alreadycleaned"], p["word"]))

img <- "thering.jpg"
lab <- ""
meme(img, lab, "thering.jpg", inset = sentimentpic)
nrc_meme <- image_read("~/Lordofthering/Rmdfiles/thering.jpg")
plot(nrc_meme)
```

```{r}
library(tibble)
library(dplyr)
library(ggplot2)
library(memery)
library(magick)
table <- tibble(alreadycleaned)
table %>% right_join(p, c("alreadycleaned" = "word")) %>% group_by(sentiment) %>% summarise(wordcount = n()) %>% ungroup() %>% mutate(sentiment = reorder(sentiment, wordcount)) 


table %>% right_join(p, c("alreadycleaned" = "word")) %>% group_by(sentiment) %>% summarise(wordcount = n()) %>% ungroup() %>% mutate(sentiment = reorder(sentiment, wordcount)) %>%
  ggplot(aes(sentiment, wordcount, fill = -wordcount)) + geom_col() + guides(fill = FALSE)+ labs(x = NULL, y = "Word Count") + scale_y_continuous(limits = c(0, 15000)) + ggtitle("The Lord of the Rings Sentiment") + coord_flip()


sentimentpic <- table %>% right_join(p, c("alreadycleaned" = "word")) %>% group_by(sentiment) %>% summarise(wordcount = n()) %>% ungroup() %>% mutate(sentiment = reorder(sentiment, wordcount)) %>%
  ggplot(aes(sentiment, wordcount, fill = -wordcount)) + geom_col() + guides(fill = FALSE)+ labs(x = NULL, y = "Word Count") + scale_y_continuous(limits = c(0, 15000)) + ggtitle("The Lord of the Rings Sentiment") + coord_flip()

#ggsave("sentimentpic.jpg")
#join <- right_join(table, p , by = (table["alreadycleaned"], p["word"]))

imgs <- "lofthering.jpg"
lab <- ""
meme(imgs, lab, "lofthering.jpg", inset = sentimentpic)
#nrc_meme <- image_read("")
nrc_meme <- image_read("~/Lordofthering/Rmdfiles/lofthering.jpg")
plot(nrc_meme)
```

```{r}
library(reshape2)
library(wordcloud)
library(RColorBrewer)
library(dplyr)
words<- table %>% right_join(p, c("alreadycleaned" = "word")) 
table %>% right_join(p, c("alreadycleaned" = "word")) %>%
  count(alreadycleaned, sentiment, sort = TRUE) %>%
  acast(alreadycleaned ~ sentiment, value.var = "n", fill = 0) %>%
  comparison.cloud(colors = c("blue", "red", "purple", "green", "gray", "orange", "green", "brown", "pink", "black"),
                   max.words = 70)
```
```{r}
library(reshape2)
library(wordcloud)
library(RColorBrewer)
library(dplyr)
words<- table %>% right_join(p, c("alreadycleaned" = "word")) 
table %>% right_join(p, c("alreadycleaned" = "word")) %>%
  count(alreadycleaned, sentiment, sort = TRUE) %>%
  acast(alreadycleaned ~ sentiment, value.var = "n", fill = 0) %>% 
  
```

```{r}
library(tidytext)
sentiments
```

```{r}
get_sentiments("afinn")
```

```{r}
get_sentiments("bing")
```

```{r}
get_sentiments("nrc")
```
```{r}
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
book1 <- Corpus(VectorSource(Book1_TheRingsetsout))
  
```

book1
```{r}
library(tidyr)
book1 <- Corpus(VectorSource(Book1_TheRingsetsout))
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
texts1 <- tm_map(book1, toSpace, "/")
texts1 <- tm_map(book1, toSpace, "@")
texts1 <- tm_map(book1, toSpace, "\\|")
texts1 <- tm_map(book1, toSpace, "?")
texts1 <- tm_map(book1, toSpace, "!")
texts1 <- tm_map(book1, toSpace, "," )
texts1 <- tm_map(book1, toSpace, " " )
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
docs1 <- tm_map(book1, toSpace, "/")
docs1 <- tm_map(book1, toSpace, "@")
docs1 <- tm_map(book1, toSpace, "\\|")
docs1 <- tm_map(book1, toSpace, "\u0099")

docs1 <- tm_map(docs1, content_transformer(tolower))
# Remove numbers
docs1 <- tm_map(docs1, removeNumbers)
# Remove english common stopwords
#docs <- tm_map(docs, removeWords, stopwords("english"))
# Remove your own stop word
# specify your stopwords as a character vector
#docs <- tm_map(docs, removeWords, c("blabla1", "blabla2")) 
# Remove punctuations
docs1 <- tm_map(docs1, removePunctuation)
# Eliminate extra white spaces
docs1 <- tm_map(docs1, stripWhitespace)
# Text stemming
docs1 <- tm_map(docs1, stemDocument)



docsv1 <- Corpus(VectorSource(docs1))
docsrsw1 <- tm_map(docsv1, removeWords, stopwords('english'))

docs1 <- tm_map(docsrsw1, stripWhitespace)
dtm1 <- TermDocumentMatrix(docsrsw1)
m1 <- as.matrix(dtm1)
v1 <- sort(rowSums(m1),decreasing=TRUE)
d1 <- data.frame(word = names(v1),freq=v1)
head(d1, 10)
set.seed(1234)
wordcloud(words = d1$word, freq = d1$freq, min.freq = 1,
          max.words=200, random.order=FALSE, rot.per=0.35, 
          colors=brewer.pal(8, "Paired"))
barplot(d1[1:10,]$freq, las = 2, names.arg = d1[1:10,]$word,
        col ="skyblue", main ="Most frequent words",
        ylab = "Word frequencies")



```

```{r}
get_sentiments("afinn")
get_sentiments("bing")
get_sentiments("nrc")
```
```{r}
titles <- c("The Ring sets out", "The War of the Ring", "The Ring goes East",
            "The Ring goes South", "The End of the Third Age", "The Treason of Isengard")


books <- list(Book1_TheRingsetsout, Book2_TheRinggoessouth, Book3_TheTreasonofIsengard,
           Book4_TheRinggoesEast, Book5_TheWaroftheRing, Book6_TheEndoftheThirdAge)

series <- tibble()

for(i in seq_along(titles)) {
        
        clean <- tibble(chapter = seq_along(books[[i]]),
                        text = books[[i]]) %>%
             unnest_tokens(word, text) %>%
             mutate(book = titles[i]) %>%
             select(book, everything())

        series <- rbind(series, clean)
}
```

books <- list(Book1_TheRingsetsout, Book2_TheRinggoessouth, Book3_TheTreasonofIsengard,
           Book4_TheRinggoesEast, Book5_TheWaroftheRing, Book6_TheEndoftheThirdAge)
  
series <- tibble()

for(i in seq_along(titles)) {
        
        clean <- tibble(chapter = seq_along(books[[i]]),
                        text = books[[i]]) %>%
             unnest_tokens(word, text) %>%
             mutate(book = titles[i]) %>%
             select(book, everything())

        series <- rbind(series, clean)
}

```{r}
series$book <- factor(series$book, levels = rev(titles))

series
```

```{r}
series %>%
        right_join(get_sentiments("nrc")) %>%
        filter(!is.na(sentiment)) %>%
        count(sentiment, sort = TRUE)
```
```{r}
library(tidyr)
series %>%
        group_by(book) %>% 
        mutate(word_count = 1:n(),
               index = word_count %/% 500 + 1) %>% 
        inner_join(get_sentiments("bing")) %>%
        count(book, index = index , sentiment) %>%
        ungroup() %>%
        spread(sentiment, n, fill = 0) %>%
        mutate(sentiment = positive - negative,
               book = factor(book, levels = titles)) %>%
        ggplot(aes(index, sentiment, fill = book)) +
          geom_bar(alpha = 0.5, stat = "identity", show.legend = FALSE) +
          facet_wrap(~ book, ncol = 2, scales = "free_x")
```

```{r}
afinn <- series %>%
        group_by(book) %>% 
        mutate(word_count = 1:n(),
               index = word_count %/% 500 + 1) %>% 
        inner_join(get_sentiments("afinn")) %>%
        group_by(book, index) %>%
        summarise(sentiment = sum(score)) %>%
        mutate(method = "AFINN")

bing_and_nrc <- bind_rows(series %>%
                  group_by(book) %>% 
                  mutate(word_count = 1:n(),
                         index = word_count %/% 500 + 1) %>% 
                  inner_join(get_sentiments("bing")) %>%
                  mutate(method = "Bing"),
          series %>%
                  group_by(book) %>% 
                  mutate(word_count = 1:n(),
                         index = word_count %/% 500 + 1) %>%
                  inner_join(get_sentiments("nrc") %>%
                                     filter(sentiment %in% c("positive", "negative"))) %>%
                  mutate(method = "NRC")) %>%
        count(book, method, index = index , sentiment) %>%
        ungroup() %>%
        spread(sentiment, n, fill = 0) %>%
        mutate(sentiment = positive - negative) %>%
        select(book, index, method, sentiment)
#We now have an estimate of the net sentiment (positive - negative) in each chunk of the novel text for each #sentiment lexicon. Letâ€™s bind them together and plot them.

bind_rows(afinn, 
          bing_and_nrc) %>%
        ungroup() %>%
        mutate(book = factor(book, levels = titles)) %>%
  ggplot(aes(index, sentiment, fill = method)) +
  geom_bar(alpha = 0.8, stat = "identity", show.legend = FALSE) +
  facet_grid(book ~ method)
```

```{r}
bing_word_counts <- series %>%
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort = TRUE) %>%
  ungroup()
```

```{r}
bing_word_counts
```

```{r}
bing_word_counts %>%
        group_by(sentiment) %>%
        top_n(10) %>%
        ggplot(aes(reorder(word, n), n, fill = sentiment)) +
          geom_bar(alpha = 0.8, stat = "identity", show.legend = FALSE) + 
          facet_wrap(~sentiment, scales = "free_y") +
          labs(y = "Contribution to sentiment", x = NULL) +
          coord_flip()
```

```{r}
library(tm)
booklower <- tm_map(book1, content_transformer(tolower))
book1removestop <- tm_map(booklower, removeWords, stopwords('english'))

bookremovewhite <- tm_map(book1removestop, stripWhitespace)
#bookstem <- tm_map(bookremovewhite, stemDocument)
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
text1 <- tm_map(book1, toSpace, '[[:punct:]]')
bookremovewhitte <- gsub(' +', ' ', text1)
#bookremovewhitte
#bookremovepun <- gsub('[[:punct:]]+', ' ', bookremovewhite) 
#bookremovewhitte <- gsub(' +', ' ', bookremovepun)
#bookremovewhitte
#str(bookremovewhitte)
bookremovewhittte <- Corpus(VectorSource(bookremovewhitte))
dtm <- TermDocumentMatrix(bookremovewhittte)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
head(d, 10)
```


```{r}
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
text1 <- tm_map(book1, toSpace, "/")
text1 <- tm_map(book1, toSpace, "@")
text1 <- tm_map(book1, toSpace, "\\|")
text1 <- tm_map(book1, toSpace, "?")
text1 <- tm_map(book1, toSpace, "!")
text1 <- tm_map(book1, toSpace, "," )
text1 <- tm_map(book1, toSpace, " " )
docb1 <- tm_map(text1, toSpace, "/")
docb1 <- tm_map(text1, toSpace, "," )
docb1 <- tm_map(text1, toSpace, " " )
docb1 <- tm_map(text1, toSpace, "@")
docb1 <- tm_map(text1, toSpace, "\\|")
docb1 <- tm_map(text1, toSpace, "\u0099")
```

```{r}
# Convert the text to lower case
docb1s <- tm_map(docb1, content_transformer(tolower))
# Remove numbers
docb1s <- tm_map(docb1, removeNumbers)
# Remove english common stopwords
#docs <- tm_map(docs, removeWords, stopwords("english"))
# Remove your own stop word
# specify your stopwords as a character vector
#docs <- tm_map(docs, removeWords, c("blabla1", "blabla2")) 
# Remove punctuations
docb1s <- tm_map(docb1, removePunctuation)
# Eliminate extra white spaces
docb1s <- tm_map(docb1, stripWhitespace)
# Text stemming
docb1s <- tm_map(docb1, stemDocument)
```

```{r}
#inspect(docb1s)
docb1sws <- tm_map(docb1, removeWords, stopwords('english'))
```

```{r}
inspect(docb1sws)
```
```{r}
series %>%
        anti_join(stop_words) %>%
        group_by(book) %>%
        count(word, sort = TRUE) %>%
        top_n(10)
```

```{r}
series %>%
        anti_join(stop_words) %>%
        group_by(book) %>%
        count(word, sort = TRUE) %>%
        top_n(10) %>%
        ungroup() %>%
        mutate(book = factor(book, levels = titles),
               text_order = nrow(.):1) %>%
        ggplot(aes(reorder(word, text_order), n, fill = book)) +
          geom_bar(stat = "identity") +
          facet_wrap(~ book, scales = "free_y") +
          labs(x = "NULL", y = "Frequency") +
          coord_flip() +
          theme(legend.position="none")
```

```{r}
ring_pct <- series %>%
        anti_join(stop_words) %>%
        count(word) %>%
        transmute(word, all_words = n / sum(n))


frequency <- series %>%
        anti_join(stop_words) %>%
        count(book, word) %>%
        mutate(book_words = n / sum(n)) %>%
        left_join(ring_pct) %>%
        arrange(desc(book_words)) %>%
        ungroup()
        
frequency
```

```{r}
ggplot(frequency, aes(x = book_words, y = all_words, color = abs(all_words - book_words))) +
        geom_abline(color = "gray40", lty = 2) +
        geom_jitter(alpha = 0.1, size = 2.5, width = 0.3, height = 0.3) +
        geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5) +
        scale_x_log10(labels = scales::percent_format()) +
        scale_y_log10(labels = scales::percent_format()) +
        scale_color_gradient(limits = c(0, 0.001), low = "red", high = "gray75") +
        facet_wrap(~ book, ncol = 3) +
        theme(legend.position="none") +
        labs(y = "The lord of the ring series", x = NULL)

```

```{r}
frequency %>%
        group_by(book) %>%
        summarize(correlation = cor(book_words, all_words),
                  p_value = cor.test(book_words, all_words)$p.value)
```

1) Term Frequencies

Computing term frequency
```{r}
book_words <- series %>%
        count(book, word, sort = TRUE) %>%
        ungroup()

series_words <- book_words %>%
        group_by(book) %>%
        summarise(total = sum(n))

book_words <- left_join(book_words, series_words)

book_words
```

```{r}
book_words %>%
        mutate(ratio = n / total) %>%
        ggplot(aes(ratio, fill = book)) +
        geom_histogram(show.legend = FALSE) +
        scale_x_log10() + stat_bin(bins = 30, binwidth = 0.02) +
        facet_wrap(~ book, ncol = 3)
```

2) Zipf's Law

```{r}
freq_by_rank <- book_words %>%
        group_by(book) %>%
        mutate(rank = row_number(),
               `term freq` = n / total)
        

ggplot(freq_by_rank, aes(rank, `term freq`, color = book)) +
        geom_line() +
        scale_x_log10() +
        scale_y_log10()
```

3) Inverse Document Frequency and tf-idf

```{r}
book_words <- book_words %>%
        bind_tf_idf(word, book, n)

book_words
```

```{r}
book_words %>%
        arrange(desc(tf_idf))
```

```{r}
book_words %>%
        arrange(desc(tf_idf)) %>%
        mutate(word = factor(word, levels = rev(unique(word))),
               book = factor(book, levels = titles)) %>% 
        group_by(book) %>%
        top_n(15, wt = tf_idf) %>%
        ungroup() %>%
        ggplot(aes(word, tf_idf, fill = book)) +
        geom_bar(stat = "identity", alpha = .8, show.legend = FALSE) +
        labs(title = "Highest tf-idf words in the Lord of the Rings series",
             x = NULL, y = "tf-idf") +
        facet_wrap(~book, ncol = 3, scales = "free") +
        coord_flip()
```

Word Relationships

```{r}
titles <- c("The Ring sets out", "The War of the Ring", "The Ring goes East",
            "The Ring goes South", "The End of the Third Age", "The Treason of Isengard")

books <- list(Book1_TheRingsetsout, Book2_TheRinggoessouth, Book3_TheTreasonofIsengard,
           Book4_TheRinggoesEast, Book5_TheWaroftheRing, Book6_TheEndoftheThirdAge)
  
series <- tibble()

for(i in seq_along(titles)) {
        
        clean <- tibble(chapter = seq_along(books[[i]]),
                        text = books[[i]]) %>%
             unnest_tokens(bigram, text, token = "ngrams", n = 2) %>%
             mutate(book = titles[i]) %>%
             select(book, everything())

        series <- rbind(series, clean)
}

```
```{r}
series$book <- factor(series$book, levels = rev(titles))

series
```

```{r}
series %>%
        count(bigram, sort = TRUE)
```

```{r}
series %>%
        separate(bigram, c("word1", "word2"), sep = " ") %>%
        filter(!word1 %in% stop_words$word,
               !word2 %in% stop_words$word) %>%
        count(word1, word2, sort = TRUE)
```

```{r}
series %>%
        separate(bigram, c("word1", "word2"), sep = " ") %>%
        filter(!word1 %in% stop_words$word,
               !word2 %in% stop_words$word) %>%
        count(book, word1, word2, sort = TRUE) %>%
        unite("bigram", c(word1, word2), sep = " ") %>%
        group_by(book) %>%
        top_n(10) %>%
        ungroup() %>%
        mutate(book = factor(book) %>% forcats::fct_rev()) %>%
        ggplot(aes(drlib::reorder_within(bigram, n, book), n, fill = book)) +
        geom_bar(stat = "identity", alpha = .8, show.legend = FALSE) +
        drlib::scale_x_reordered() +
        facet_wrap(~ book, ncol = 2, scales = "free") +
        coord_flip()
```

Analyzing n-grams

```{r}
(bigram_tf_idf <- series %>%
        count(book, bigram, sort = TRUE) %>%
        bind_tf_idf(bigram, book, n) %>%
        arrange(desc(tf_idf))
)
```

```{r}
bigram_tf_idf %>%
        group_by(book) %>%
        top_n(15, wt = tf_idf) %>%
        ungroup() %>%
        mutate(book = factor(book) %>% forcats::fct_rev()) %>%
        ggplot(aes(drlib::reorder_within(bigram, tf_idf, book), tf_idf, fill = book)) +
        geom_bar(stat = "identity", alpha = .8, show.legend = FALSE) +
        labs(title = "Highest tf-idf bi-grams in the Lord of the Rings series",
             x = NULL, y = "tf-idf") +
        drlib::scale_x_reordered() +
        facet_wrap(~book, ncol = 2, scales = "free") +
        coord_flip()
```

```{r}
series %>%
        separate(bigram, c("word1", "word2"), sep = " ") %>%
        filter(word1 == "not") %>%
        count(book, word1, word2, sort = TRUE)
```

```{r}
AFINN <- get_sentiments("afinn")

(nots <- series %>%
        separate(bigram, c("word1", "word2"), sep = " ") %>%
        filter(word1 == "not") %>%
        inner_join(AFINN, by = c(word2 = "word")) %>%
        count(word2, score, sort = TRUE) 
)

```

```{r}
nots %>%
        mutate(contribution = n * score) %>%
        arrange(desc(abs(contribution))) %>%
        head(20) %>%
        ggplot(aes(reorder(word2, contribution), n * score, fill = n * score > 0)) +
        geom_bar(stat = "identity", show.legend = FALSE) +
        xlab("Words preceded by 'not'") +
        ylab("Sentiment score * # of occurrances") +
        coord_flip()
```

```{r}
negation_words <- c("not", "no", "never", "without")

(negated <- series %>%
                separate(bigram, c("word1", "word2"), sep = " ") %>%
                filter(word1 %in% negation_words) %>%
                inner_join(AFINN, by = c(word2 = "word")) %>%
                count(word1, word2, score, sort = TRUE) %>%
                ungroup()
)
```

```{r}
negated %>%
        mutate(contribution = n * score) %>%
        arrange(desc(abs(contribution))) %>%
        group_by(word1) %>%
        top_n(10, abs(contribution)) %>%
        ggplot(aes(drlib::reorder_within(word2, contribution, word1), contribution, fill = contribution > 0)) +
        geom_bar(stat = "identity", show.legend = FALSE) +
        xlab("Words preceded by 'not'") +
        ylab("Sentiment score * # of occurrances") +
        drlib::scale_x_reordered() +
        facet_wrap(~ word1, scales = "free") +
        coord_flip()

```

```{r}
library(igraph)

(bigram_graph <- series %>%
        separate(bigram, c("word1", "word2"), sep = " ") %>%
        filter(!word1 %in% stop_words$word,
               !word2 %in% stop_words$word) %>%
        count(word1, word2, sort = TRUE) %>%
        unite("bigram", c(word1, word2), sep = " ") %>%
        filter(n > 20) %>%
        graph_from_data_frame()
)
```

```{r}
library(ggraph)
set.seed(123)

a <- grid::arrow(type = "closed", length = unit(.15, "inches"))

ggraph(bigram_graph, layout = "fr") +
        geom_edge_link() +
        geom_node_point(color = "lightblue", size = 5) +
        geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
        theme_void()
```

```{r}
(ps_words <- tibble(entirebook = seq_along(TheLordoftheRings),
                  text = TheLordoftheRings) %>%
        unnest_tokens(word, text) %>%
        filter(!word %in% stop_words$word))
```

```{r}
library(widyr)

(word_pairs <- ps_words %>%
        pairwise_count(word, entirebook, sort = TRUE))
```

```{r}
word_pairs %>% 
        filter(item1 == "time")
```

```{r}
(word_cor <- ps_words %>%
   group_by(word) %>%
   filter(n() >= 20) %>%
   pairwise_cor(word, entirebook) %>%
   filter(!is.na(correlation)))
```

```{r}
word_cor %>%
  filter(item1 == "frodo") %>%
  arrange(desc(correlation))
```

```{r}
set.seed(123)

ps_words %>%
  group_by(word) %>%
  filter(n() >= 20) %>%
  pairwise_cor(word, entirebook) %>%
  filter(!is.na(correlation),
         correlation > .65) %>%
  graph_from_data_frame() %>%
  ggraph(layout = "fr") +
  geom_edge_link(aes(edge_alpha = correlation), show.legend = FALSE) +
  geom_node_point(color = "lightblue", size = 5) +
  geom_node_text(aes(label = name), repel = TRUE) +
  theme_void()
```

```{r}
library(tm)

ps_dtm <- VectorSource(Book1_TheRingsetsout) %>%
  VCorpus() %>%
  DocumentTermMatrix(control = list(removePunctuation = TRUE,
                                    removeNumbers = TRUE,
                                    stopwords = TRUE))

inspect(ps_dtm)

```

```{r}
terms <- Terms(ps_dtm)
head(terms)
```

```{r}
ps_tidy <- tidy(ps_dtm)
ps_tidy
```

```{r}
ps_tidy %>%
  group_by(document) %>%
  top_n(5) %>%
  ungroup() %>%
  mutate(document = factor(as.numeric(document), levels = 1:17)) %>%
  ggplot(aes(drlib::reorder_within(term, count, document), count)) +
  geom_bar(stat = "identity") +
  xlab("Top 5 Common Words") +
  drlib::scale_x_reordered() +
  coord_flip() +
  facet_wrap(~ document, scales = "free")
```

```{r}
ps_dfm <- quanteda::dfm(TheLordoftheRings, verbose = FALSE)

ps_dfm
```

```{r}
ps_tidy <- tidy(ps_dfm)
ps_tidy
```

```{r}
ps_tidy %>%
  cast_dfm(term, document, count)

ps_tidy %>%
  cast_dtm(term, document, count)

ps_tidy %>%
  cast_sparse(term, document, count) %>%
  dim
```

```{r}
ps_corpus <- VectorSource(philosophers_stone) %>%
  VCorpus()
ps_corpus
```

```{r}
ps_corpus[[1]]
```

```{r}
ps_tidy <- tidy(ps_corpus)
ps_tidy
```

```{r}
data("acq")
acq
```

```{r}
acq[[1]]
```

```{r}
(tidy_acq <- tidy(acq))
```






